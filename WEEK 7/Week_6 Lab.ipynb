{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Operations on NumPy arrays\n",
        "We will first represent our data into **array**. Once our data are represented as arrays, they are **operated** on to either build solutions during the learning stage or to make predictions during the deployment stage.\n",
        "\n",
        "NumPY arrays are a convenient way to represent homogeneous datasets, e.g, datasets where all the attributes are of the same type, for instance `float` or `int`. In this lab, we will explore some of the most important **mathematical operations** on NumPy arrays. These operations will allow us to implement solutions based on **matrix algebra** and are known as **vectorised operations**, as they operate on a whole array (*vectors*) rather than on individual values at a time.\n",
        "\n",
        "The main operations that we will cover include:\n",
        "- Arithmetic operations (addition, subtraction, etc)\n",
        "- Matrix transposition\n",
        "- Matrix multiplication\n",
        "- Matrix inversion\n",
        "\n",
        "We can use these operations to obtain the least squares solution of several linear and polynomial models. **Before attempting the lab**, make sure that you have read and understood the notes on mathematical notation and basic maths and have reviewed the lecture notes on regression."
      ],
      "metadata": {
        "id": "1ZbGa0B8x3lh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Basic operations on arrays\n",
        "\n",
        "In this section we will review some of the most important operations on arrays.\n",
        "\n",
        "First, we will use `np.array` to create a **row vector** `x_r`, specifically a vector consisting of 1 row and 3 columns, and then we will use `np.transpose` to **transpose** it into a **column vector** `x_c` consisting of 3 rows and 1 column:"
      ],
      "metadata": {
        "id": "NVnAcn5zy4Tw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "uw5lPsGjzVaZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nwXJ9geXu4_V"
      },
      "outputs": [],
      "source": [
        "x_r = np.array([0, 1, 2], ndmin=2)\n",
        "print(\"Vector x_r is:\\n\", x_r)\n",
        "print(\"The shape of x_r is :\", x_r.shape)\n",
        "x_c = np.transpose(x_r)\n",
        "print(\"\\nVector x_c is:\\n\", x_c)\n",
        "print(\"The shape of x_c is:\", x_c.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that we could have used `x_c = np.array([[0], [1], [2]])` to produce a column vector directly, instead of producing a row vector and then using transposition."
      ],
      "metadata": {
        "id": "cu-XHpjN0oON"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_c = np.array([[0], [1], [2]])\n",
        "print(\"Vector x_c is:\\n\", x_c)"
      ],
      "metadata": {
        "id": "tYIL1zHk0VhT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's now use **column stacking** to create a ** Matrix**. We will explore how we can design a custom Matrix:\n"
      ],
      "metadata": {
        "id": "e-Krkaff0-XO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's create a **matrix X** that consists 2 columns, specifically a column of ones and another column contains x_c"
      ],
      "metadata": {
        "id": "QzP-HxGx4IDg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.column_stack([np.ones(x_c.shape), x_c])\n",
        "print(\"The design matrix X is:\\n\", X)\n",
        "print(\"\\nThe shape of the matrix X is\", X.shape)"
      ],
      "metadata": {
        "id": "ge9t0LWz1H-X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question:1\n",
        "Now, using column stacking, create a Matrix that contains 3 columns. The first column is specifically a column with ones, the second column contains an array that hold values: 2, 3, 4 and the third column contains an array that holds values: 8, 9, 10."
      ],
      "metadata": {
        "id": "DaqS7pWh5RyM"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yyq8uVof6Om9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is not always the case that the first column value has to be ones, but it can be any value we want to bring when we do column stacking. However, if you remember your seminar today, you should be able to recall that we use a matrix in calculations regarding Regression analysis, called **Design Matrix**, in which the first column consisted of only ones. We will be using this matrix we created just now to make Design Matrices frm here onwards."
      ],
      "metadata": {
        "id": "8TGy8zCX-QQG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Regression: Linear Regression"
      ],
      "metadata": {
        "id": "MDjqlBXYvC5e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's explore a simple regression problem. First, we will define and plot our training dataset, which consists of 10 samples described by one predictor `x` and one label `y`."
      ],
      "metadata": {
        "id": "w1uyTSd9AvVb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "ImiUpO_9CbOk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.array([0.3000, -0.7700, 0.9000, -0.0400, 0.7400, -0.5800, -0.9200, -0.2100, -0.5400, 0.6800], ndmin=2).T\n",
        "y = np.array([1.1492,  0.3582, 1.9013,  0.9487, 1.3096,  0.9646,  0.1079,  1.1262,  0.6131, 1.0951], ndmin=2).T\n",
        "\n",
        "plt.plot(x, y, 'o', label=\"Training Data\")\n",
        "plt.xlabel(\"x (attribute)\", fontsize=18)\n",
        "plt.ylabel(\"y (label)\", fontsize=18)\n",
        "plt.xlim(-1,1) \n",
        "plt.ylim(-0.5,2.5)\n",
        "plt.grid(alpha=0.2)\n",
        "plt.legend(fontsize=12)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "uDDpKWPaA5D3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "HTbJvH1gR-kD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that here we have obtained the transpose of two row vectors using `.T` instead of `np.transpose`. Both options are equivalent. The most important point is that `x` and `y` are two column vectors consisting of 10 rows and 1 column, i.e. 10x1 vectors:"
      ],
      "metadata": {
        "id": "-iezJhcvDP_Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(x.shape)\n",
        "print(y.shape)"
      ],
      "metadata": {
        "id": "swrgzCLLDlOs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Also, we can load dataset in a dataframe from other sources and plot them to get an idea of how they are distributed in the attribute space."
      ],
      "metadata": {
        "id": "S3jixqP8EH8i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "WOUFokxdEejV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_sal_age = pd.read_csv('./Age Salary Dataset.csv')\n",
        "df_sal_age.info()"
      ],
      "metadata": {
        "id": "LwPVAS5aIHQ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print (df_sal_age.columns)\n",
        "print (df_sal_age.index)"
      ],
      "metadata": {
        "id": "nbaHwxUlKQhV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictor= df_sal_age.iloc [:,0]\n",
        "x=np.array(predictor, ndmin=2).T\n",
        "print (x)\n",
        "label= df_sal_age.iloc[:,1]\n",
        "y=np.array(label, ndmin=2).T\n",
        "print (y)\n",
        "\n",
        "plt.plot(x, y, 'o', label=\"Training Data\")\n",
        "plt.xlabel(\"x (age)\", fontsize=18)\n",
        "plt.ylabel(\"y (salary)\", fontsize=18)\n",
        "plt.xlim(15,70)\n",
        "plt.ylim(5000,100000)\n",
        "plt.grid(alpha=0.2)\n",
        "plt.legend(fontsize=12)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mrpntqmcLMq7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Linear Regression: Optimal Solution\n",
        "\n",
        "Theoritically, as you have already seen that the best fit line can be obtained with the \"Least Squares\" solution, let's see how we can implement it programmatically."
      ],
      "metadata": {
        "id": "J2iXC-6FVC5p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Least squares\n",
        "\n",
        "Given a family of regression models, the least squares solution is the model that minimises the mean square error on our training dataset. Consider the folowing **multiple linear model**:\n",
        "\n",
        "$\\begin{equation}\n",
        "f(x) = w_0 + w_1 x_1 + ... + w_K x_K\n",
        "\\end{equation}$\n",
        "\n",
        "where $x_1, ..., x_K$ are the predictors and $w_0, ..., w_K$ are the model's parameters. If we have a dataset consisting of $N$ samples, we can obtain the parameters of the least squares solution using the **normal equations** \n",
        "\n",
        "$\\begin{equation}\n",
        "\\mathbf{w} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}\n",
        "\\end{equation}$\n",
        "\n",
        "where $\\mathbf{w}=[w_1, ... , w_K]^T$ are the parameters of the model, $\\mathbf{y}=[y_0, ..., y_N]^T$ are the true labels in the dataset and $\\mathbf{X}$ is the design matrix. The least squares solution for **simple linear regression** can be obtained following an identical approach. Given a linear model\n",
        "\n",
        "$\\begin{equation}\n",
        "f(x) = w_0 + w_1 x \n",
        "\\end{equation}$\n",
        "\n",
        "\n",
        "As you can see from the normal equations, obtaining the least square solution involves: \n",
        "\n",
        "- **Arrays** ($\\mathbf{w}$, $\\mathbf{y}$ and $\\mathbf{X}$).\n",
        "- **Transposition**.\n",
        "- **Matrix inversion**.\n",
        "- **Matrix multiplication**. "
      ],
      "metadata": {
        "id": "Ww9IAoZcSCaQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's obtain the least squares solution for a simple linear model $f(x) = w_0 + w_1 x$. We need to calculate the design matrix first and then use the normal equation. We will show the calculations involved in the normal equations step by step:\n",
        "\n",
        "Step 1:  $X^TX$\n",
        "\n",
        "Step 2: $(X^TX)^{-1}$\n",
        "\n",
        "Step 3: $(X^TX)^{-1}X^T$"
      ],
      "metadata": {
        "id": "aRCpuItmXDQ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.column_stack([np.ones(x.shape), x])\n",
        "print(x.shape)\n",
        "print(\"The design matrix is:\\n\", X)\n",
        "print(X.shape)\n",
        "\n",
        "XTX = np.dot(X.T, X) # Step 1\n",
        "print(XTX.shape)\n",
        "\\\n",
        "\n",
        "XTX_inv = np.linalg.inv(XTX) # Step 2\n",
        "XTX_invXT = np.dot(XTX_inv, X.T) # Step 3\n",
        "\n",
        "w = np.dot(XTX_invXT, y)\n",
        "print(\"The 2 parameters of the least squares linear solution are\\n\", w)"
      ],
      "metadata": {
        "id": "g7f3HtFLXh_Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have the parameters $w_0$ and $w_1$ of the linear model, we can use them to carry out predictions. Let's predict the salary for people of age 20,23,29,49,50,70. Since the predicted values will lie on the trend-line producted by our least squares solution function, we will use thos value tp plot the line in the attribute space."
      ],
      "metadata": {
        "id": "ucyHVIjhbEfj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_arr = np.array([20,23,29,49,50,70]).T\n",
        "X_arr = np.column_stack([np.ones(x_arr.shape), x_arr])\n",
        "y_arr = np.dot(X_arr, w)\n",
        "\n",
        "print(x_arr)\n",
        "print(y_arr)\n",
        "\n",
        "plt.plot(x, y, 'o', label=\"Dataset\")\n",
        "plt.plot(x_arr, y_arr, label=\"Linear prediction\")\n",
        "plt.xlabel(\"x (age)\", fontsize=18)\n",
        "plt.ylabel(\"y (salary)\", fontsize=18)\n",
        "plt.xlim(15,80)\n",
        "plt.ylim(5000,100000)\n",
        "plt.grid(alpha=0.2)\n",
        "plt.legend(fontsize=12)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "x2t0zW6ek59s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question:2\n",
        "Let's go back to the first regression example you have seen today, where x was the predictor and y was the label with training data as:\n",
        "\n",
        "\n",
        "x = [0.3000, -0.7700, 0.9000, -0.0400, 0.7400, -0.5800, -0.9200, -0.2100, -0.5400, 0.6800]\n",
        "\n",
        "y=[1.1492,  0.3582, 1.9013,  0.9487, 1.3096,  0.9646,  0.1079,  1.1262,  0.6131, 1.0951]\n",
        "\n",
        "You are now to find the optimal solution for this given dataset and display the linear regression line with proper plotting of the datapoints that are given below:\n",
        "x_test=[0.2000, 0.990, 0.3450, -0.9910, -0.2480, 0.1000, 0.9120, 0.7450, 0.3450, -0.1000]\n"
      ],
      "metadata": {
        "id": "VhkiEtOqqSgm"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nDITVzNf0NKc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7K3abHeO0g6J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Sp6X6jnx0n4v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question: 3\n",
        "\n",
        "Now, you are given with another dataset that contains the price (in thousand Â£) and size (in sq. feet) of the houses. \n",
        "\n",
        "Find the prices for the houses of the following sizes:\n",
        "2000, 1200, 1480, 2200, 1890, 1050, 2390\n",
        "\n",
        "Also, draw the trend-line using the predicted values."
      ],
      "metadata": {
        "id": "iBCRupXB2Tsx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "aGKzdJoO-m7v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6mv_Ipc9_VVf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Aru6VvXNBLut"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kidADSR5ArCe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Multiple Linear Regression\n",
        "If there are are more than one predictor involved then the regression we do is a Multiple Linear Regression.\n",
        "\n",
        "Assuming we are now predicting house prices which is related to two predictors, where one of them is size and the other one is no. of Bedrooms."
      ],
      "metadata": {
        "id": "cQtVpzQfEGjq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_house_bed_price = pd.read_csv('./House Price  Bedroom Dataset.csv')\n",
        "df_house_bed_price.info()"
      ],
      "metadata": {
        "id": "jOEmdF-uEzLh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictor1= df_house_bed_price.iloc[:,1]\n",
        "predictor2= df_house_bed_price.iloc[:,2]\n",
        "x1=np.array(predictor1, ndmin=2).T\n",
        "x2=np.array(predictor2, ndmin=2).T\n",
        "print (x1)\n",
        "print (x2)\n",
        "\n",
        "label= df_house_price.iloc[:,0]\n",
        "y=np.array(label, ndmin=2).T\n",
        "print (y)"
      ],
      "metadata": {
        "id": "aqEUbb8SFzOR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, using the predictors x1 & x2 we can make a new Design Matrix that has N x 4 dimension. Unsing this new Design Matrix and the given labels we can get the parameter values for the least squares solution for this multiple linear regression."
      ],
      "metadata": {
        "id": "z8tJXyQXPHzd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.column_stack([np.ones(x1.shape), x1, x2])\n",
        "print(\"The design matrix is:\\n\", X)\n",
        "XTX = np.dot(X.T, X) # Step 1\n",
        "XTX_inv = np.linalg.inv(XTX) # Step 2\n",
        "XTX_invXT = np.dot(XTX_inv, X.T) # Step 3\n",
        "\n",
        "w = np.dot(XTX_invXT, y)\n",
        "print(\"The 3 parameters of the least squares linear solution are\\n\", w)"
      ],
      "metadata": {
        "id": "EYCAyuSUKUwB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have the parameters for the least squares solution for this multiple linear regression, we can carry out predictions such finding prices for houses with the following no. of beds and size touples:\n",
        "(2, 1300), (5, 3000), (1, 1100), (2, 1350), (3, 2500) "
      ],
      "metadata": {
        "id": "HZg4ma0OPl_r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x1_arr = np.array([2, 5, 1, 2, 3]).T\n",
        "x2_arr = np.array([1300, 3000, 1100, 1350,2500]).T\n",
        "\n",
        "X_arr = np.column_stack([np.ones(x1_arr.shape), x1_arr,x2_arr])\n",
        "y_arr = np.dot(X_arr, w)\n",
        "\n",
        "#print(x_arr)\n",
        "print(\"So the house prices are:\")\n",
        "print(y_arr)"
      ],
      "metadata": {
        "id": "HyMt8C70QWr-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}