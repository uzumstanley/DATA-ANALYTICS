{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#K-Nearest Neighbour\n",
        "K-NN or K Nearest Neighbour is a easy to understand but one of the powerful and topmost technique can be used for /data Analysis. We will mainly be exploring this technique as Classification approach in this module."
      ],
      "metadata": {
        "id": "-jaZQLAj6Aj-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Scikit-Learn Library:\n",
        "\n",
        "\n",
        "- This is one of the popular and powerful libraries in Python used for predictive Analysis\n",
        "-Open Source, Accessible and Reusable in various ways\n",
        "-Various **Popular example Datasets** are available within this library for practicing different Techniques, such as, Regression and Classification**\n",
        "- Scikit-Learn is built on **NumPy** , **SciPy** and **Matplotlib**, which you are already familiarized with"
      ],
      "metadata": {
        "id": "FPHaLYZH6gLF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this lab we will use the well-known Iris dataset to build and analyse the performance of multi-class classifiers. We will first consider a **kNN** classifier and analyse the impact of changing the value of k on the resulting decision regions.\n",
        "\n",
        "In general we won't be able to visualise our datasets and error surfaces in the same way, as the predictor space will have a high dimensionality (i.e. there will be many predictors). Even though we won't be able to visualise decision regions in a 10D or 200D space, decision regions still exist and our methods will work in exactly the same way. The trick is to understand and carry out all our mental experiments in low-dimensional spaces, and then assume everything works in the same way in high-dimensional spaces."
      ],
      "metadata": {
        "id": "xQNB_Pd-Yuv6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Iris dataset\n",
        "\n",
        "The Iris flower dataset is a classic dataset used to identify three flower species based on features describing their [sepals](https://en.wikipedia.org/wiki/Sepalhttps://) and [petals](https://en.wikipedia.org/wiki/Petal). Let's load it first and print its description."
      ],
      "metadata": {
        "id": "AF1jFQKpZge3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As read in the description, the Iris dataset consists of 150 samples described by 5 attributes. Four of them will be used used as predictors (the flowers' features) and the fifth one will be used as a discrete label. \n",
        "\n",
        "The predictors are:\n",
        "- Sepal length.\n",
        "- Sepal width.\n",
        "- Petal length. \n",
        "- Petal width.\n",
        "\n",
        "The label can take on three different values, in other words, there are three classes in the Iris dataset, which are:\n",
        "- Setosa.\n",
        "- Versicolor.\n",
        "- Virginica.\n",
        "\n",
        "Each class corresponds to a different flower species. Hence, the Iris dataset can be used to build machine learning models that identify one of these flowers species by looking at the 4 features describing their petals and sepals. \n"
      ],
      "metadata": {
        "id": "vHQOMkBJaSuN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "\n",
        "iris = datasets.load_iris()\n",
        "\n",
        "print(iris.DESCR)"
      ],
      "metadata": {
        "id": "7xevslyhZ875"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's load the predictors and label into two separate NumPy arrays `x_iris` and `y_iris` and check their shape:"
      ],
      "metadata": {
        "id": "bMVXRwexat0D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_iris = iris.data\n",
        "y_iris = iris.target\n",
        "\n",
        "print(\"The shape of the NumPy array x_iris containing the predictors is:\", x_iris.shape)\n",
        "print(\"The shape of the NumPy array y_iris containing the label is:\",y_iris.shape)\n",
        "print (y_iris)"
      ],
      "metadata": {
        "id": "EBQlo4_vasK9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we will use the first two predictors (sepal length and sepal width), as we can easily visualise 2D predictors space. Let's store the two predictors in a new NumPy array:\n",
        "\n",
        "(We are doing index splicing in this step. You can check the uploaded \"iris dataset\" on Moodle to understand how the splicing worked on the dataset. Remember that, we don't need to load the data this time as this dataset has been imported from scikit-learn library.)"
      ],
      "metadata": {
        "id": "xrnnW7c763zh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_iris_sepal = iris.data[:, :2] \n",
        "\n",
        "print(\"The first two predictors are\", iris.feature_names[:2])\n",
        "print(\"The shape of the NumPy array x_iris_sepal containing the first two pre predictors is\", x_iris_sepal.shape)"
      ],
      "metadata": {
        "id": "5VsUEthodZ0-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As expected, the shape of the NumPy array `x_iris_sepal` is `(150, 2)`, which means there are 150 samples desribed by 2 predictors. \n",
        "\n",
        "Let's plot the dataset in the predictor space. We will use 3 different symbols to represent each of the 3 different values that the label `y_iris` can take on, namely 0 (setosa), 1 (versicolor) and 2 (virginica)."
      ],
      "metadata": {
        "id": "aqgx2sQHeU5Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(y_iris==0) # It works like a simple if-else condition here."
      ],
      "metadata": {
        "id": "HnjnNEOEekY-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(7, 7))\n",
        "\n",
        "scatter = plt.scatter(x_iris_sepal[y_iris==0,0], x_iris_sepal[y_iris==0,1], s=50 , color= '#FF0000', label='{:d} ({:s})'.format(0, iris.target_names[0]))\n",
        "scatter = plt.scatter(x_iris_sepal[y_iris==1,0], x_iris_sepal[y_iris==1,1], s=50 , color= '#00FF00', label='{:d} ({:s})'.format(1, iris.target_names[1]))\n",
        "scatter = plt.scatter(x_iris_sepal[y_iris==2,0], x_iris_sepal[y_iris==2,1], s=50 , color= '#0000FF', label='{:d} ({:s})'.format(2, iris.target_names[2]))\n",
        "\n",
        "plt.legend(fontsize=12)\n",
        "plt.xlabel(iris.feature_names[0], fontsize=14)\n",
        "plt.ylabel(iris.feature_names[1], fontsize=14)\n",
        "plt.xlim(4,8.5)\n",
        "plt.ylim(1.5,5)\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jIaV5bJ1fKH1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plotting the dataset in the predictor space reveals that the three classes are not linearly separable, i.e. we cannot create straight boundaries in the predictor space that separate samples from different classes. Can you visualise a non-linear boundary capable of separating the three classes? \n",
        "\n",
        "\n",
        "Let's split the dataset into a training dataset and a validation dataset that we will use to assess the deployent performance of our models. In general we will need to carefully determine how many samples should be used for training and for validation. However for the purpose of this lab, let's assume that 75 samples are sufficient for training and validation purposes. After splitting, let's plot the training and validation datasets."
      ],
      "metadata": {
        "id": "SFEQDyZ5gq_l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Here we split our dataset into training and validation datasets\n",
        "x_train = x_iris_sepal[::2,:] # train data set (both predictors)\n",
        "y_train = y_iris[::2] # train data set (labels)\n",
        "x_val = x_iris_sepal[1::2,:] # test data set (both predictors)\n",
        "y_val = y_iris[1::2] # test data set (labels)\n",
        "\n",
        "\n",
        "# The rest of this cell is used to plot the training and validation datasets\n",
        "plt.figure(figsize=(7, 7))\n",
        "\n",
        "scatter = plt.scatter(x_train[y_train==0,0], x_train[y_train==0,1], s=50 , color= '#FF0000', label='{:d} ({:s})'.format(0, iris.target_names[0]))\n",
        "scatter = plt.scatter(x_train[y_train==1,0], x_train[y_train==1,1], s=50 , color= '#00FF00', label='{:d} ({:s})'.format(1, iris.target_names[1]))\n",
        "scatter = plt.scatter(x_train[y_train==2,0], x_train[y_train==2,1], s=50 , color= '#0000FF', label='{:d} ({:s})'.format(2, iris.target_names[2]))\n",
        "\n",
        "plt.title(\"Training dataset\")\n",
        "plt.legend(fontsize=12)\n",
        "plt.xlabel(iris.feature_names[0], fontsize=14)\n",
        "plt.ylabel(iris.feature_names[1], fontsize=14)\n",
        "plt.xlim(4,8.5)\n",
        "plt.ylim(1.5,5)\n",
        "plt.grid(True)\n",
        "\n",
        "plt.figure(figsize=(7, 7))\n",
        "\n",
        "scatter = plt.scatter(x_val[y_val==0,0], x_val[y_val==0,1], s=50 , color= '#FF0000', label='{:d} ({:s})'.format(0, iris.target_names[0]))\n",
        "scatter = plt.scatter(x_val[y_val==1,0], x_val[y_val==1,1], s=50 , color= '#00FF00', label='{:d} ({:s})'.format(1, iris.target_names[1]))\n",
        "scatter = plt.scatter(x_val[y_val==2,0], x_val[y_val==2,1], s=50 , color= '#0000FF', label='{:d} ({:s})'.format(2, iris.target_names[2]))\n",
        "\n",
        "plt.title(\"Validation dataset\")\n",
        "plt.legend(fontsize=12)\n",
        "plt.xlabel(iris.feature_names[0], fontsize=14)\n",
        "plt.ylabel(iris.feature_names[1], fontsize=14)\n",
        "plt.xlim(4,8.5)\n",
        "plt.ylim(1.5,5)\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "iUj0Ltp3hTJL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# K-NN classifiers\n",
        "\n",
        "K-NN algorithm is an instance-based method that assigns a new sample to the **majotiry class** amongst the k closest training samples. From a statistical point of view, we can see K-NN as a method that creates **posterior probabilities** by identifying the closest training samples and obtaining the fraction of samples belonging to each class.\n",
        "\n",
        "We could easily implement kNN ourselves, however we will use the implementation provided by the [scikit-learn](https://scikit-learn.org/stable/) library.  \n",
        "\n",
        "In the cell below we will create several kNN classifiers for different values of k, will show the resulting decision regions and finally will obtain their validation accuracy. Note that k shouldn't be seen as a parameter of our model, but rather as a **hyperparameter**. As you know, parameters are adjusted (for instance the intercept and slope of a straight line), while hyperparameters define families of models that behave very differently. \n",
        "\n",
        "The kNN algorithm is a strange one, in that there are **no parameters to adjust** and there is one hyperparameter."
      ],
      "metadata": {
        "id": "G9aC2JQchxqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import widgets\n",
        "from sklearn import neighbors\n",
        "#from matplotlib.colors import ListedColormap\n",
        "\n",
        "k_values = range(1,75,6)\n",
        "tb = widgets.TabBar([str(k) for k in k_values])\n",
        "\n",
        "#cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])\n",
        "\n",
        "accuracy = dict.fromkeys(k_values)\n",
        "\n",
        "for k in k_values:\n",
        "  with tb.output_to(str(k), select= (k < 2)):\n",
        "\n",
        "    # First we create the kNN model\n",
        "    knn = neighbors.KNeighborsClassifier(n_neighbors=k)\n",
        "    knn.fit(x_train, y_train)\n",
        "\n",
        "\n",
        "\n",
        "    # Finally we calculate the validation accuracy\n",
        "    y_val_pred = knn.predict(x_val)\n",
        "    accuracy[k] = np.sum(y_val==y_val_pred)/len(y_val)\n",
        "\n",
        "    print(\"The validation accuracy for k=\", k, \"is \", accuracy[k])\n",
        "\n",
        "\n",
        "# Here we predict the value of the validation accuracy as a function of k\n",
        "plt.figure(figsize=(7, 7))\n",
        "plt.plot(k_values, list(accuracy.values()), '--*', linewidth=2)\n",
        "plt.xlabel(\"k\", fontsize=12)\n",
        "plt.ylabel(\"Validation accuracy\", fontsize=12)\n",
        "plt.grid(alpha=0.2)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "N4ci2IyOi735"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question:1\n",
        "Now, based on this simple validation approach, which value of \"K\" would you select and why?"
      ],
      "metadata": {
        "id": "HexEkAkilqEh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once, you decide about the value for \"K\", you should now be able to predict a suitable classification for an unseen datapoint. \n",
        "\n",
        "Let's now find a classification for (5,1), where 5 is the Sepal length and, 1 is Sepal width."
      ],
      "metadata": {
        "id": "ce4XypEvm0Lt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "model = KNeighborsClassifier(n_neighbors=25)\n",
        "\n",
        "# Train the model using the training sets\n",
        "model.fit(x_iris_sepal,y_iris)\n",
        "\n",
        "#Predict Output\n",
        "predicted= model.predict([[5,1]]) \n",
        "print(predicted)"
      ],
      "metadata": {
        "id": "sPAZO2a1p0BD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The output is \"1\" and \"1\" is equivalent to \"versicolor\", as we encoded it previously."
      ],
      "metadata": {
        "id": "ae3uIir6q-80"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question:2\n",
        "Now, use a K-NN Model to fit \"petal height\" and \"petal width\" predictors from the same dataset and predict classification for (4,2), where 4 is the Petal length and, 2 is Petal width."
      ],
      "metadata": {
        "id": "fCFnVaqHrqK7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_iris_petal = iris.data[:, 2:] \n",
        "print (x_iris_petal.shape)\n",
        "print (x_iris_petal)"
      ],
      "metadata": {
        "id": "7j9yYxsrcPqf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}